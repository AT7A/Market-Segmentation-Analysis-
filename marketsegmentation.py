# -*- coding: utf-8 -*-
"""MarketSegmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TXOte5pZs8FG4zo3eeZzJ4noMNjTXiyg

Market Segmentation Analysis on McDonalds Data
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage

df = pd.read_csv('/content/mcdonalds.csv')

df

df['Index'] = df.index
df

df.shape

df.info()

df.head(3)

df.describe()

df.dtypes

df.isnull().sum()

df['Gender'].value_counts()

mcd_x = (df.iloc[:, :11] == "Yes").astype(int)
col_means = np.round(np.mean(mcd_x, axis= 0), 2)
col_means

from sklearn.preprocessing import LabelEncoder

def labelling(a):
    df[a] = LabelEncoder().fit_transform(df[a])
    return df
category  = ['yummy', 'convenient', 'spicy', 'fattening', 'greasy', 'fast', 'cheap',
       'tasty', 'expensive', 'healthy', 'disgusting']

# Specify the columns to encode
columns_to_encode = df.columns[:11]
label_encoder = LabelEncoder()

for column in columns_to_encode:
    df[column] = label_encoder.fit_transform(df[column])

df

b = df.loc[:,category].values
b

#Principal component analysis
from sklearn.decomposition import PCA
from sklearn import preprocessing

mcd_pca = preprocessing.scale(b)

pca = PCA(n_components=11)
pc = pca.fit_transform(b)
names = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11']
pf =pd.DataFrame(data= pc, columns= names)
pf

# Print standard deviation, Proportion of variance and Cumulative proportion
print("Standard deviation")
print(pca.explained_variance_)
print("\n Proportion of variance")
print(pca.explained_variance_ratio_)
print("\n Cumulative proportion")
print(np.cumsum(pca.explained_variance_ratio_))

#Results from principal components analysis
loading = pca.components_
digit = pca.n_features_
pclist = ["PC" + str(i) for i in range(1, digit + 1)]
loading_df = pd.DataFrame(loading.T, columns=pclist)
loading_df['variable'] = mcd_x.columns.values
loading_df = loading_df.set_index('variable')
loading_df

!pip install bioinfokit

from bioinfokit.visuz import cluster
# get PC scores
pca_scores = PCA().fit_transform(b)

# get 2D biplot
cluster.biplot(cscore=pca_scores, loadings=loading, labels=df.columns.values, var1=round(pca.explained_variance_ratio_[0]*100, 2),
    var2=round(pca.explained_variance_ratio_[1]*100, 2),show=True,dim=(10,6))

target_col = 'Gender'
features = df.drop(columns=[target_col])
target = df[target_col]
target

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=0)

df['Like'] = pd.to_numeric(df['Like'], errors='coerce')

features = pd.get_dummies(features)

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)
scaled_features

# Dimensionality reduction using PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(scaled_features)
pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])
pca_df

#Kmean clustring
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_l = kmeans.fit_predict(pca_df)
kmeans_l

# Perform Agglomerative (Hierarchical) Clustering
clustering = AgglomerativeClustering(n_clusters=4)
clusterlabels = clustering.fit_predict(pca_df)

np.random.seed(1234)
best_model = max(
    (KMeans(n_clusters=k, random_state=np.random.randint(0, 10000)).fit(mcd_x)
     for k in range(2, 9)),
    key=lambda m: silhouette_score(mcd_x, m.labels_)
)
labels = LabelEncoder().fit_transform(best_model.labels_)
labels

# Visualize dendrogram for hierarchical clustering
plt.figure(figsize=(10, 7))
linkage_matrix = linkage(pca_df, method='ward')
dendrogram(linkage_matrix)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Assuming you have a list of silhouette scores for each cluster count
silhouette_scores = [silhouette_score(mcd_x, KMeans(n_clusters=k, random_state=np.random.randint(0, 10000)).fit(mcd_x).labels_)
                     for k in range(2, 9)]

# Plotting
plt.figure(figsize=(8, 5))
plt.bar(range(2, 9), silhouette_scores, color = 'skyblue')
plt.xlabel("Number of Segments (Clusters)")
plt.ylabel("sum of within cluster distances")
plt.title("Silhouette Scores for Different Numbers of Clusters")
plt.xticks(range(2, 9))
plt.show()

from sklearn.metrics import adjusted_rand_score
from sklearn.utils import resample

# Set seed for reproducibility
np.random.seed(1234)
true_labels = cluster_labels

def bootFlexclust(data, k_values, nrep=10, nboot=100):
    results = []

    for k in k_values:
        for _ in range(nrep):
            boot_samples = resample(data, n_samples=len(data), replace=True)
            kmeans = KMeans(n_clusters=k, random_state=0).fit(boot_samples)
            labels = kmeans.labels_

            # Calculate adjusted Rand index
            if true_labels is not None:
                ari = adjusted_rand_score(true_labels, labels)
                results.append([k, ari])

    return pd.DataFrame(results, columns=['Number of Segments', 'Adjusted Rand Index'])

k_values = range(2, 9)

# Run bootFlexclust
result = bootFlexclust(mcd_x, k_values)

# Create a boxplot
plt.figure(figsize=(10, 10))
sns.boxplot(x='Number of Segments', y='Adjusted Rand Index', data=result)
plt.xlabel("Number of Segments")
plt.ylabel("Adjusted Rand Index")
plt.title("Boxplot of Adjusted Rand Index by Number of Segments")
plt.grid()
plt.show()

df['Segment'] = kmeans_labels
df

df['VisitFrequency'] = df['VisitFrequency'].replace({
    'Never': 0,
    'Once a year': 1,
    'Every three months': 2,
    'Once a month': 3,
    'Once a week': 4,
    'More than once a week': 5
})

df['Like']= df['Like'].replace({
    'I hate it!-5': '-5',
    'I love it!+5':'+5'
})

df

#plot
sns.catplot(x="Like", y="Age",data=df, orient="v", height= 5, aspect= 2, palette="Set1",kind="swarm")
plt.title('Like and Age')
plt.show()

#Histogram of the each attributes
plt.rcParams['figure.figsize'] = (9,13)
df.hist()
plt.show()

#plot
grouped_data = df['VisitFrequency'].value_counts().reset_index()
grouped_data.columns = ['VisitFrequency', 'Likes']

# Plotting the bar chart
plt.figure(figsize=(10, 5))
plt.bar(grouped_data['VisitFrequency'], grouped_data['Likes'], color='green')

# Adding labels and title
plt.xlabel('VisitFrequency')
plt.ylabel('Likes')
plt.title('Likes \'s VisitFrequency')
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

features = df.select_dtypes(include='number')  # Modify as needed

# Fit KMeans model (MD.k4 equivalent)
kmeans = KMeans(n_clusters=4, random_state=42)
df['kmeans_cluster'] = kmeans.fit_predict(features)

# Assuming MD.m4 is a mixture model; for example, we can do another clustering
# Here we'll also use KMeans for simplicity
mixture_model = KMeans(n_clusters=4, random_state=42)
df['mixture_cluster'] = mixture_model.fit_predict(features)

# Create a contingency table
contingency_table = pd.crosstab(df['kmeans_cluster'], df['mixture_cluster'])
contingency_table.index += 1
contingency_table.columns += 1

contingency_table

n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
clusters_kmeans = kmeans.fit_predict(mcd_x)

# Fit Gaussian Mixture Model (GMM)
gmm = GaussianMixture(n_components=n_clusters, random_state=0)
gmm.fit(mcd_x)
clusters_gmm = gmm.predict(mcd_x)
contingency_table = pd.crosstab(clusters_kmeans, clusters_gmm, rownames=['kmeans'], colnames=['mixture'])

print(contingency_table)

gmm = GaussianMixture(n_components=4, random_state=42)
gmm_labels = gmm.fit_predict(pca_df)

n_clusters = 4  # Adjust as needed
gmm = GaussianMixture(n_components=n_clusters, random_state=0)
gmm.fit(mcd_x)

# Calculate log-likelihood
log_likelihood = gmm.score(mcd_x) * mcd_x.shape[0]  # score returns log-likelihood per sample
df = gmm.n_components * (mcd_x.shape[1] + 1)  # Degrees of freedom

print(f"log Likelihood: {log_likelihood:.3f} (df={df})")

